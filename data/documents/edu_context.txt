This article explores the advantages and disadvantages of providing context to Large Language Models to improve performance (instead of fine-tuning). It also explores the use of Vector Databases as a context information source.

Large Language Models (LLMs) are pre-trained on a massive amount of training examples at an extraordinary cost. This pre-training computes the underlying parameters (weights and biases) in the neural network in order to minimise differences (or losses) between the modelâ€™s prediction and the actual training example. The large number of weights (tens of billions+) and diverse training data sometimes allows the model to generalise well.
